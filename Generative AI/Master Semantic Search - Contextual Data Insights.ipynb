{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b187106-9217-4de6-a8e5-c05567146673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp312-cp312-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b681f9-af1c-4826-9c78-6501e91f00a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow>=2.0.0) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow>=2.0.0) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.0.0) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow>=2.0.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow>=2.0.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.0.0) (0.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow>=2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3107a9c3-a303-4fa7-9baa-7bee349f2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (3.20.3)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.65.4)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow, tf-keras, tensorflow-hub\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "Successfully installed keras-3.7.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-hub-0.16.1 tf-keras-2.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c417d8-e8b6-48ec-92b7-1cef3b2ff6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing other required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde7015-e0e3-4cca-97c6-24ee284e60c8",
   "metadata": {},
   "source": [
    "# Understanding Semantic Search\n",
    "Imagine you’re looking for something on the internet, like the best place to get pizza in your city. You type in “best pizza near me,” and traditional search engines give you results based on keywords like “best,” “pizza,” and “near.” These results might show you a list of pizza places, but they don't necessarily understand the meaning behind what you're really asking — what if you were looking for vegetarian pizza, or a pizza place with a specific ambiance? This is where semantic search comes in, and it’s a real game-changer.\n",
    "\n",
    "# What is Semantic Search?\n",
    "Semantic search goes beyond simple keywords and tries to understand the meaning behind your search, the context in which you’re asking, and the intent behind the words. It’s like having a conversation with the search engine instead of just typing words and hoping it guesses correctly.\n",
    "\n",
    "* Here's how semantic search improves the search experience:\n",
    "It doesn't just match words, it matches concepts. So if you search for “best pizza place,” the engine might understand that you're not just looking for pizza recipes, but for actual places to eat pizza nearby.\n",
    "It understands the relationship between words. For example, it knows that \"doctor\" and \"physician\" are the same thing and might show you results for either term.\n",
    "It also learns from your behavior and adjusts its responses based on what you like and how you search.\n",
    "\n",
    "# How Semantic Search Works? \n",
    "Think of the search engine like a smart assistant. Here’s what it does when you type in a query:\n",
    "* Getting the Gist: Instead of just looking at the words you typed, the search engine tries to understand the real meaning behind your question. For example, if you typed \"apple,\" it needs to decide whether you're looking for fruit or a tech product. It looks at context to figure that out.\n",
    "* Making Connections: The search engine looks at all the possible ways words can be related. It might know that “car” and “automobile” mean the same thing, or “vacation” and “holiday” are connected. It’s like connecting the dots between words that have similar meanings.\n",
    "* Picking the Best: Now, the search engine sorts through tons of information (like a librarian who knows every book in the library) and finds what most closely matches your query, based on what it thinks you really mean.\n",
    "\n",
    "# The Technical Side of Semantic Search \n",
    "This part involves something called Natural Language Processing (NLP) and vectors.\n",
    "\n",
    "## Vectors: The Language of Semantic Search\n",
    "Imagine if every word or sentence could be turned into a series of numbers. These numbers, called vectors, help the search engine understand the meaning behind the words. It’s like giving each word a unique numerical fingerprint that represents its meaning.\n",
    "### Creating Vectors: \n",
    "The first thing the search engine does is turn every word or sentence into a vector. For example, the word “cat” might turn into a vector like [0.5, 1.2, 0.7], and “dog” might turn into [0.4, 1.1, 0.8]. These numbers are not random — they represent the meanings of the words.\n",
    "\n",
    "### Measuring Similarity: \n",
    "Once we have these numbers (vectors), the search engine uses math to figure out how close or similar two words or sentences are. So, if you searched for “cat,” the engine could compare its vector to other vectors in its database and find words that are close in meaning.\n",
    "\n",
    "### Using Vectors for Search: \n",
    "When you type in a search query, the search engine turns your query into a vector, too. It then looks through its massive collection of vectors to find the ones that are closest to yours. The closest vectors are likely to be the most relevant results for your search.\n",
    "\n",
    "# How Vectors Power the Search\n",
    "Vectors are really helpful because they capture the meaning behind the words, not just the words themselves. Here’s how it works step by step:\n",
    "1. Vectorization: When you type in your query, the engine turns your words into a vector — think of it as a unique set of numbers representing your words.\n",
    "2. Indexing: The search engine has a huge index (like a library catalog) of vectors for all the information it has. It stores them so it can find them quickly when you search.\n",
    "3. Retrieval: The search engine then compares your query’s vector to all the stored vectors, and finds the ones that are closest. These are the results that are most semantically (meaning-wise) similar to what you wanted.\n",
    "\n",
    "# Why is This Better Than Traditional Search?\n",
    "With traditional search engines, you might get results based only on the words you typed, which can miss the meaning. But with semantic search, the engine can understand the intent behind your query, like recognizing synonyms, context, or even user preferences. So if you search for “how to fix a flat tire,” the results will focus on repair guides and tools, not just the words “fix” and “tire” in isolation.\n",
    "\n",
    "# Summary\n",
    "In short, semantic search helps search engines get smarter and more contextually aware. Instead of just matching words, it understands what you really mean, making your search more accurate and relevant. It’s like upgrading from a basic search engine to one that thinks and understands language, just like we do. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439a5a0-52aa-40ea-a7aa-ef091673daa9",
   "metadata": {},
   "source": [
    "# Understanding Vectorization and Indexing\n",
    "If you've ever used a search engine to find something, you might have noticed that it doesn't always give you exactly what you're looking for, especially if your query is complex. This is where vectorization and indexing come into play. They are essential parts of building a semantic search engine that truly understands the meaning of your query. These techniques work on two powerful tools that help make it happen: Universal Sentence Encoder (USE) and FAISS.\n",
    "\n",
    "# What is the Universal Sentence Encoder (USE)?\n",
    "Think of the Universal Sentence Encoder (USE) like a super-smart translator that turns sentences into numbers. But why would we want to turn sentences into numbers? The reason is that computers don’t understand language the way humans do. To make sense of text, we need to break it down into something the computer can understand — and that’s where numbers come in.\n",
    "\n",
    "## Working of USE:\n",
    "1. Language Understanding: The USE doesn’t just look at words in isolation. It understands the meaning of sentences by looking at context — how words fit together. For example, the words “bank” and “river bank” have different meanings, and USE can tell the difference based on the surrounding words.\n",
    "2. Versatility: The USE is trained on a lot of different types of data, so it can handle all kinds of sentences, whether they're simple or complicated. It’s like a general knowledge expert that can talk about many topics.\n",
    "3. Speed: Once USE is trained, it can quickly convert any sentence into a vector (a list of numbers), making it efficient for fast searches.\n",
    "\n",
    "* When USE takes in a sentence, it doesn’t just look at each word individually. It takes into account the relationship between the words and their order. For example:\n",
    "\"I love pizza\" is different from \"Pizza I love,\" even though they have the same words.\n",
    "USE understands that “cat” and “kitten” are related, but “cat” and “dog” are different.\n",
    "\n",
    "Here’s what USE does step by step:\n",
    "1. Analyzes the Words: It looks at each word in the sentence and how they connect with other words.\n",
    "2. Understands the Context: It also considers where each word appears in the sentence. For example, the word \"run\" can mean \"to move fast\" or \"to manage something\" depending on the sentence.\n",
    "3. Creates Vectors: After analyzing the sentence, USE converts the information into a vector, which is a list of numbers. This vector captures the meaning of the entire sentence.\n",
    "So, when you type a query into a search engine, it turns your sentence into a vector, and the search engine uses these vectors to understand the meaning of your request.\n",
    "\n",
    "# What is FAISS and what does it do?\n",
    "Once we’ve got our vectors, we need to store and search through them efficiently. That’s where FAISS comes in. FAISS (which stands for Facebook AI Similarity Search) is a tool that helps search through these vectors quickly and accurately. It’s like a librarian that not only knows where every book is but can also find the most relevant books to what you’re looking for in a fraction of a second.\n",
    "\n",
    "Here’s how FAISS helps:\n",
    "\n",
    "1. Efficient Searching: FAISS uses special algorithms to search through huge collections of vectors quickly. Imagine if you had to go through thousands of books to find one with a similar topic — FAISS does this in the blink of an eye.\n",
    "2. Scalability: FAISS can handle really large datasets, even ones that are too big to fit in a computer’s memory. This means it works well for databases that contain millions or even billions of vectors.\n",
    "3. Accuracy: FAISS doesn’t just search quickly; it also provides highly accurate results by organizing the vectors smartly.\n",
    "\n",
    "# How does FAISS work?\n",
    "FAISS works in three steps:\n",
    "1. Index Building: First, FAISS takes all the vectors and organizes them. It places similar vectors next to each other in a way that makes it faster to search through them. Imagine a library where books on the same topic are grouped together.\n",
    "\n",
    "2. Searching: When you search for something, FAISS doesn’t go through every single vector. It only looks at the ones that are most likely to be relevant. This makes the search much faster.\n",
    "\n",
    "3. Retrieving Results: Finally, FAISS finds the closest vectors to our query and retrieves the most relevant results. These results are not just text that matches our words but meaningful connections to what we were really looking for.\n",
    "\n",
    "* Putting it All Together: USE + FAISS = Powerful Semantic Search\n",
    "\n",
    "* USE helps convert sentences into vectors — these are numbers that represent the meaning of the sentences.\n",
    "* FAISS organizes and indexes these vectors so that when you search, it can quickly find the most relevant ones.\n",
    "* Together, they create a semantic search engine that doesn't just match keywords but understands the meaning behind your search query. This allows the search engine to return results that are much more relevant and accurate, even for complex or vague queries.\n",
    "\n",
    "* So, instead of just searching for “pizza near me,” a semantic search engine might understand that you're looking for the best vegetarian pizza places or restaurants with good ambiance based on how you phrase your query and the context of your previous searches.\n",
    "\n",
    "* In short, USE and FAISS work together to help search engines understand language at a deeper level and deliver smarter, more relevant results in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dcf40-0b21-44ee-a2dc-254889792a25",
   "metadata": {},
   "source": [
    "# 20 Newsgroups dataset\n",
    "\n",
    "The 20 Newsgroups dataset is like a big collection of messages that people posted in online discussion forums called newsgroups. Think of it as a set of digital conversations, where people are talking about all sorts of topics. There are around 20,000 of these messages, and they are grouped into 20 categories or themes, like sports, science, politics, religion, and so on.\n",
    "\n",
    "# Why is this dataset interesting?\n",
    "Imagine you have a giant library of conversations. Some are about sports, like \"How to improve your basketball skills,\" while others are about tech, like \"What’s the best programming language?\" This dataset is fascinating because it’s full of real messages written by real people, so it captures all the quirks of how we actually write and communicate. This includes slang, typos, and even context that isn’t always obvious. It’s like trying to sort through a stack of letters to figure out what each one is about—it can be challenging, but also very rewarding. \n",
    "\n",
    "# What is done with this dataset in the course?\n",
    "We’re going to take this collection of messages and do the below mentioned tasks: \n",
    "\n",
    "* create a tool that can quickly find and compare messages based on their meaning. This process has a few steps:\n",
    "\n",
    "1. Looking at the Data: First, we’ll open up the dataset to see what’s inside. This is like flipping through a few letters in the pile to get a sense of what they’re about. We’ll check out the structure—how the messages are stored and what kinds of information they include.\n",
    "\n",
    "2. Cleaning the Data: Next, we’ll clean up the text. For example, some messages might have extra information like email addresses, repeated words, or irrelevant symbols that don’t help us understand the meaning. Cleaning is like removing dust and clutter to make sure we’re only focusing on the important parts.\n",
    "\n",
    "3. Turning Text into Numbers: This might sound strange, but computers understand numbers much better than words. So, we’ll use a tool called the Universal Sentence Encoder. Think of it as a translator that takes each message and turns it into a special number code, capturing the overall meaning of the message. For example, a message about \"soccer training\" and one about \"football practice\" might get very similar number codes, even though the words are different.\n",
    "\n",
    "4. Making a Search Tool: Once we have all these number codes, we’ll use a powerful tool called FAISS. It’s like a super-efficient filing system that can quickly match any message to similar ones. So, if you search for \"basketball tips,\" it can instantly find all the conversations that are related, even if the exact words don’t match.\n",
    "\n",
    "5. Why is this useful?\n",
    "Imagine you’re trying to find advice about a specific topic, like the best hiking trails, but the exact phrase \"hiking trails\" isn’t used in the messages. With this tool, you could search for the idea of hiking trails, and it would still find the right discussions for you! It’s all about understanding the meaning behind the words, not just the words themselves.\n",
    "\n",
    "* This project, is all about how a big pile of messy text can be transformed into something organized and powerful—a tool that helps you search and understand ideas, even when the language is tricky or complicated. It’s like creating a smart assistant for navigating through conversations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841c6552-a823-4cdd-92a2-7702fffbaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8faf0c6c-0814-468f-af76-0c9afba0e47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ae3b2e6-974d-4ece-9f4b-8954dab633f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample post 1:\n",
      "\n",
      "(\"From: lerxst@wam.umd.edu (where's my thing)\\n\"\n",
      " 'Subject: WHAT car is this!?\\n'\n",
      " 'Nntp-Posting-Host: rac3.wam.umd.edu\\n'\n",
      " 'Organization: University of Maryland, College Park\\n'\n",
      " 'Lines: 15\\n'\n",
      " '\\n'\n",
      " ' I was wondering if anyone out there could enlighten me on this car I saw\\n'\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/\\n'\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition,\\n'\n",
      " 'the front bumper was separate from the rest of the body. This is \\n'\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years\\n'\n",
      " 'of production, where this car is made, history, or whatever info you\\n'\n",
      " 'have on this funky looking car, please e-mail.\\n'\n",
      " '\\n'\n",
      " 'Thanks,\\n'\n",
      " '- IL\\n'\n",
      " '   ---- brought to you by your neighborhood Lerxst ----\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample post 2:\n",
      "\n",
      "('From: guykuo@carson.u.washington.edu (Guy Kuo)\\n'\n",
      " 'Subject: SI Clock Poll - Final Call\\n'\n",
      " 'Summary: Final call for SI clock reports\\n'\n",
      " 'Keywords: SI,acceleration,clock,upgrade\\n'\n",
      " 'Article-I.D.: shelley.1qvfo9INNc3s\\n'\n",
      " 'Organization: University of Washington\\n'\n",
      " 'Lines: 11\\n'\n",
      " 'NNTP-Posting-Host: carson.u.washington.edu\\n'\n",
      " '\\n'\n",
      " 'A fair number of brave souls who upgraded their SI clock oscillator have\\n'\n",
      " 'shared their experiences for this poll. Please send a brief message '\n",
      " 'detailing\\n'\n",
      " 'your experiences with the procedure. Top speed attained, CPU rated speed,\\n'\n",
      " 'add on cards and adapters, heat sinks, hour of usage per day, floppy disk\\n'\n",
      " 'functionality with 800 and 1.4 m floppies are especially requested.\\n'\n",
      " '\\n'\n",
      " 'I will be summarizing in the next two days, so please add to the network\\n'\n",
      " \"knowledge base if you have done the clock upgrade and haven't answered this\\n\"\n",
      " 'poll. Thanks.\\n'\n",
      " '\\n'\n",
      " 'Guy Kuo <guykuo@u.washington.edu>\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample post 3:\n",
      "\n",
      "('From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\n'\n",
      " 'Subject: PB questions...\\n'\n",
      " 'Organization: Purdue University Engineering Computer Network\\n'\n",
      " 'Distribution: usa\\n'\n",
      " 'Lines: 36\\n'\n",
      " '\\n'\n",
      " 'well folks, my mac plus finally gave up the ghost this weekend after\\n'\n",
      " \"starting life as a 512k way back in 1985.  sooo, i'm in the market for a\\n\"\n",
      " 'new machine a bit sooner than i intended to be...\\n'\n",
      " '\\n'\n",
      " \"i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\n\"\n",
      " 'of questions that (hopefully) somebody can answer:\\n'\n",
      " '\\n'\n",
      " '* does anybody know any dirt on when the next round of powerbook\\n'\n",
      " \"introductions are expected?  i'd heard the 185c was supposed to make an\\n\"\n",
      " 'appearence \"this summer\" but haven\\'t heard anymore on it - and since i\\n'\n",
      " \"don't have access to macleak, i was wondering if anybody out there had\\n\"\n",
      " 'more info...\\n'\n",
      " '\\n'\n",
      " '* has anybody heard rumors about price drops to the powerbook line like the\\n'\n",
      " \"ones the duo's just went through recently?\\n\"\n",
      " '\\n'\n",
      " \"* what's the impression of the display on the 180?  i could probably swing\\n\"\n",
      " \"a 180 if i got the 80Mb disk rather than the 120, but i don't really have\\n\"\n",
      " 'a feel for how much \"better\" the display is (yea, it looks great in the\\n'\n",
      " 'store, but is that all \"wow\" or is it really that good?).  could i solicit\\n'\n",
      " 'some opinions of people who use the 160 and 180 day-to-day on if its worth\\n'\n",
      " 'taking the disk size and money hit to get the active display?  (i realize\\n'\n",
      " \"this is a real subjective question, but i've only played around with the\\n\"\n",
      " 'machines in a computer store breifly and figured the opinions of somebody\\n'\n",
      " 'who actually uses the machine daily might prove helpful).\\n'\n",
      " '\\n'\n",
      " '* how well does hellcats perform?  ;)\\n'\n",
      " '\\n'\n",
      " \"thanks a bunch in advance for any info - if you could email, i'll post a\\n\"\n",
      " 'summary (news reading time is at a premium with finals just around the\\n'\n",
      " 'corner... :( )\\n'\n",
      " '--\\n'\n",
      " 'Tom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical '\n",
      " 'Engineering\\n'\n",
      " '---------------------------------------------------------------------------\\n'\n",
      " '\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\n'\n",
      " 'Nietzsche\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 3 posts from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Sample post {i+1}:\\n\")\n",
    "    pprint(newsgroups_train.data[i])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e819e-36a9-4518-bfa6-baea9ce186a7",
   "metadata": {},
   "source": [
    "# Pre-processing the data\n",
    "\n",
    "# How do we clean up the data?\n",
    "1. Getting the Messages:\n",
    "First, we grab all the messages from the dataset and store them in a list. This is like collecting all the letters from our big pile of conversations so we can work on them one by one.\n",
    "\n",
    "2. Cleaning Each Message:\n",
    "We create a special \"cleaning recipe\" to process every message. Here’s what this recipe does:\n",
    "* Remove Email Headers: Messages often start with extra information like \"From: John@example.com.\" While this is useful in an email, it’s not important for understanding the main message, so we remove it.\n",
    "* Get Rid of Email Addresses: Some messages might include email addresses in the text, like \"Contact me at Jane@example.com.\" These don’t help us understand the topic of the message, so we remove them too.\n",
    "* Remove Punctuation and Numbers: Characters like #, $, %, and even numbers are stripped away. This way, we focus only on the words that carry meaning.\n",
    "* Make Everything Lowercase: Imagine if one message says \"Football\" and another says \"football.\" These mean the same thing, so we standardize everything by converting all the text to lowercase.\n",
    "* Clean Up Extra Spaces: Sometimes there are unnecessary spaces or line breaks in the text. We tidy these up so the text looks neat and compact.\n",
    "\n",
    "3. Applying the Recipe to All Messages:\n",
    "Now, we go through each message in our list and apply this cleaning recipe. Once we’re done, we have a new set of clean and organized messages, all ready for the next steps.\n",
    "\n",
    "# Why do we do this?\n",
    "When you’re reading a letter, your brain automatically ignores the messy parts, like typos or unnecessary details. Computers, on the other hand, can’t do that. They see every single character, so we have to help them by removing all the \"noise\" and focusing on the actual content of the message. This step is like giving the computer a clean and easy-to-read version of the conversations, which will make everything we do later much more effective.\n",
    "\n",
    "* By cleaning up the data, we’re ensuring that our search tool can understand the messages better and deliver more accurate results. It’s a simple but important step to make the whole process smoother and smarter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "372ee334-8cef-4aff-bef9-9a7d5f60f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data\n",
    "\n",
    "# Basic preprocessing of text data\n",
    "def preprocess_text(text):\n",
    "    # Remove email headers\n",
    "    text = re.sub(r'^From:.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess each document\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a40fac8-5b1d-491d-84b5-86c54dab9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Preprocessed post:\n",
      "\n",
      "subject what car is this nntppostinghost racwamumdedu organization university of maryland college park lines i was wondering if anyone out there could enlighten me on this car i saw the other day it was a door sports car looked to be from the late s early s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please email thanks il brought to you by your neighborhood lerxst\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a sample post to display\n",
    "sample_index = 0  # for example, the first post in the dataset\n",
    "\n",
    "# Print the original post\n",
    "print(\"Original post:\\n\")\n",
    "print(newsgroups_train.data[sample_index])\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the preprocessed post\n",
    "print(\"Preprocessed post:\\n\")\n",
    "print(preprocess_text(newsgroups_train.data[sample_index]))\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17aba9-cb92-4f9c-9823-ce8cb6ec162f",
   "metadata": {},
   "source": [
    "* After cleaning up the messages, the next step is to turn the words into something a computer can work with—numbers! But not just random numbers. These numbers, called \"vectors,\" are special because they capture the meaning of the messages in a way that a computer can understand. Think of it as translating the ideas in each message into a language that machines can read and compare.\n",
    "\n",
    "# How do we translate messages into numbers?\n",
    "We use a tool called the Universal Sentence Encoder (USE). This tool was built by Google and is great at taking sentences or even whole messages and turning them into meaningful numbers. \n",
    "\n",
    "## USE Working:\n",
    "1. Loading the Tool.\n",
    "* First, we load the Universal Sentence Encoder from an online library. It’s like downloading an app onto your computer.\n",
    "* The command looks like this:\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "* Once it’s loaded, it’s ready to take text and give us numbers that represent the meaning of that text.\n",
    "  \n",
    "2. Defining How We Use It\n",
    "* We write a little \"helper\" function to handle this translation for us. Let’s call it embed_text.\n",
    "* It takes a message (like \"What’s the best way to train for a marathon?\").\n",
    "* The Universal Sentence Encoder processes the message and creates a long list of numbers. These numbers are like a fingerprint for the message—unique but representing its overall meaning.\n",
    "* The result is converted into a format that’s easy to use later.\n",
    "\n",
    "3. Translating All the Messages\n",
    "* Once the helper function is ready, we give it all the cleaned messages, one at a time.\n",
    "* The Universal Sentence Encoder turns each message into its unique \"fingerprint,\" and we organize all these fingerprints into a big table, where:\n",
    "1. Each row represents one message.\n",
    "2. Each column holds part of the \"fingerprint\" (one of the numbers in the list).\n",
    "\n",
    "# Why is this important?\n",
    "By turning the text into these special \"meaningful numbers,\" we make it possible for a computer to compare messages, search through them, and even group similar ones together—without having to understand language the way humans do. It’s like giving the computer a tool to understand what each message is about, instead of just focusing on the exact words.\n",
    "\n",
    "Now that we’ve translated all our messages into this format, we’re ready to take the next step: organizing these numbers in a way that makes searching through them super fast and efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a88be0c9-f9c7-436f-98fc-5ee7df52716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def embed_text(text):\n",
    "    return embed(text).numpy()\n",
    "\n",
    "# Generate embeddings for each preprocessed document\n",
    "X_use = np.vstack([embed_text([doc]) for doc in processed_documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a04933-4dd6-4b54-82b1-4ba0657837ee",
   "metadata": {},
   "source": [
    "* Now that we’ve translated our messages into meaningful numbers (or vectors), the next challenge is to organize them in a way that allows us to quickly find similar ones. Think of it like creating a super-smart filing cabinet where we can instantly find documents that match a search, even if the words aren’t exactly the same. To do this, we use a tool called FAISS, which stands for Facebook AI Similarity Search.\n",
    "\n",
    "# Working of FAISS:\n",
    "1. Figuring Out the Size of the Vectors\n",
    "Each document (or message) is represented by a list of numbers (a vector).\n",
    "\n",
    "* For FAISS to organize these vectors, it needs to know how many numbers are in each vector—this is called the dimension.\n",
    "Example: If each vector has 512 numbers (as is typical with the Universal Sentence Encoder), the dimension is 512.\n",
    "\n",
    "* We calculate this dimension from our dataset of vectors:\n",
    "dimension = X_use.shape[1]. \n",
    "This simply tells FAISS how big each \"fingerprint\" is.\n",
    "\n",
    "2. Creating the FAISS Index\n",
    "Imagine the FAISS index as a digital filing cabinet. Each vector (representing a document) is like a file that we put into this cabinet.\n",
    "\n",
    "* We create this index using a specific method to measure similarity between vectors.\n",
    "* For this project, we use L2 distance (or Euclidean distance), which measures how far apart two vectors are. The closer the vectors, the more similar the documents they represent.\n",
    "\n",
    "* The code for creating the index looks like this:\n",
    "index = faiss.IndexFlatL2(dimension). \n",
    "Here, \"Flat\" means it’s a simple, straightforward filing cabinet—perfect for small to medium-sized datasets.\n",
    "\n",
    "3. Adding Our Vectors to the Index\n",
    "Once the filing cabinet (index) is ready, we \"file\" all our document vectors into it using:\n",
    "index.add(X_use)\n",
    "Now the index is set up with all the document fingerprints, and it’s ready to find similarities.\n",
    "\n",
    "# Why This Works So Well?\n",
    "When you search for something, FAISS looks at the vector for your query and compares it to all the vectors in its filing cabinet. It measures how similar they are and returns the closest matches in just a fraction of a second. This is what makes FAISS so powerful—it can handle lots of vectors and still be lightning fast!\n",
    "\n",
    "# A Quick Note on Choosing the Right Type of Index\n",
    "FAISS offers different types of filing cabinets depending on your needs:\n",
    "* IndexFlatL2: This is the basic filing cabinet we used. It’s simple and great for small to medium datasets.\n",
    "* IndexIVFFlat: A more advanced filing system for larger datasets. It organizes the vectors into clusters to make searching faster.\n",
    "* IndexIVFPQ: Even more efficient for really large datasets, using fancy tricks to save memory and speed up searches.\n",
    "For our project, IndexFlatL2 is perfect because it’s easy to set up and works well with the number of documents we’re handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea3bc4bb-b0b1-4654-b7d6-7176466b3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = X_use.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # Creating a FAISS index\n",
    "index.add(X_use)  # Adding the document vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0ecbf-6d19-49ec-9586-5917decd9c15",
   "metadata": {},
   "source": [
    "* Now that we've organized all our documents into a smart \"filing cabinet\" using FAISS, we can use it to search for documents that are similar to a query you enter. This is the core of a semantic search engine—the ability to find documents that are contextually similar to what you're looking for, even if the exact words aren't a match.\n",
    "\n",
    "## STEPS:\n",
    "1. Defining the Search Function\n",
    "* The search function is like a magic tool that helps us look up documents similar to a question or topic you have in mind.\n",
    "First, it takes your query like typing \"motorcycle\" into the search bar and cleans it up using the same process we used for the documents this ensures that everything is in the same format.\n",
    "* Then, it converts your query into a vector—a list of numbers that represent the meaning of the query.\n",
    "* After that, we use FAISS to search for the documents that are closest to your query's vector. FAISS looks at all the stored document vectors and finds the ones that are the most similar to the query.\n",
    "* The result is a list of documents that are semantically (meaning-wise) closest to what is asked.\n",
    "\n",
    "2. Executing a Query and Displaying Results\n",
    "Suppose we want to search for documents about motorcycles. \n",
    "## STEPS:\n",
    "1. Enter the Query: Type \"motorcycle\" into the search bar.\n",
    "2. Search Engine in Action: The search function works behind the scenes to process the word, convert it into a vector, and then asks\n",
    "* FAISS index: “Which documents are closest to this vector?”\n",
    "* Get Results: FAISS returns a list of documents it thinks are most related to the word \"motorcycle.\"\n",
    "\n",
    "# What Are the Results?\n",
    "For each of the top documents that FAISS finds:\n",
    "1. Ranking: How \"close\" the document is to your query. A closer match means it's ranked higher.\n",
    "2. Distance: This number tells us how far the document’s meaning is from the query. The lower the number, the more similar the document is.\n",
    "3. Document Text: We display the actual content of the document, so you can see exactly what’s in the document that makes it a good match.\n",
    "4. We show both:\n",
    "* The original version of the document, with all its punctuation, capitalization, etc.\n",
    "* The preprocessed version of the document, which is the cleaned-up version we use for analysis. This lets us compare how the system interpreted and processed the text.\n",
    "\n",
    "# Why is This Useful?\n",
    "This is where the magic of semantic search happens. Unlike traditional search engines that simply match words exactly, our system looks at the meaning behind the words. So if we search for \"motorcycle,\" the engine doesn’t just look for the word \"motorcycle\" in documents. Instead, it looks for documents that are about motorcycles or related to motorcycles, even if they don’t use that exact word.\n",
    "\n",
    "This is a huge advantage because it allows for more natural, human-like searching. we could search for \"bike\" or \"motorcycle repair,\" and the system will return documents about motorcycles, even if the word \"motorcycle\" doesn’t appear directly in the text. The results will be more relevant and contextually appropriate to what we were actually looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4227892b-9117-41bd-a3c7-8d3a370e8361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: (Distance: 1.0367166996002197)\n",
      "subject first bike organization freshman mechanical engineering carnegie mellon pittsburgh pa lines nntppostinghost andrewcmuedu anyone i am a serious motorcycle enthusiast without a motorcycle and to put it bluntly it sucks i really would like some advice on what would be a good starter bike for me i do know one thing however i need to make my first bike a good one because buying a second any time soon is out of the question i am specifically interested in racing bikes cbr f gsxr i know that this may sound kind of crazy considering that ive never had a bike before but i am responsible a fast learner and in love please give me any advice that you think would help me in my search including places to look or even specific bikes that you want to sell me thanks jamie belliveau\n",
      "\n",
      "Rank 2: (Distance: 1.0392435789108276)\n",
      "subject first bike organization freshman mechanical engineering carnegie mellon pittsburgh pa lines nntppostinghost poandrewcmuedu anyone i am a serious motorcycle enthusiast without a motorcycle and to put it bluntly it sucks i really would like some advice on what would be a good starter bike for me i do know one thing however i need to make my first bike a good one because buying a second any time soon is out of the question i am specifically interested in racing bikes cbr f gsxr i know that this may sound kind of crazy considering that ive never had a bike before but i am responsible a fast learner and in love please give me any advice that you think would help me in my search including places to look or even specific bikes that you want to sell me thanks jamie belliveau\n",
      "\n",
      "Rank 3: (Distance: 1.0515626668930054)\n",
      "subject re first bike organization microsoft corporation lines in recmotorcycles james leo belliveau writes i am a serious motorcycle enthusiast without a motorcycle and to put it bluntly it sucks i really would like some advice on what would be a good starter bike for me i do know one thing however i need to make my first bike a good one because buying a second any time soon is out of the question i am specifically interested in racing bikes cbr f gsxr i know that this may sound kind of crazy considering that ive never had a bike before but i am responsible a fast learner and in love responsible and in love i believe thats a contradiction in terms unless youre really brave read reckless a cc sport bike will go way faster than you dare for at least your first year of riding getting more than that really is overkill as youll never even want to use it the following bikes can be bought and repaired cheaply are easy for a novice to manage and are plenty high performance kawasaki ex honda vf interceptor suzuki gse the mph time of the ex at full throttle is way sooner than youre ready for it with something as small as a youd probably be wishing for more power pretty quickly unless its a tzr or rgv now im not saying that youre certain to kill yourself immediately with a f or a gsxr plenty of people have started riding on those bikes and done just fine what i am saying is that its a waste of money and a waste of perfectly good plastic when you drop the thing learning how to balance while stopping youll never get the throttle more than half open anyway so why spend the extra bucks chris\n",
      "\n",
      "Rank 4: (Distance: 1.0526401996612549)\n",
      "subject re new to motorcycles organization hp sonoma county srsdmwtdmid xnewsreader tin version pl lines gregory humphreys wrote greg im very new to motorcycles havent even bought one yet i was in the same position about you how do you learn if youve never ridden i took a class put on by a group called the motorcycle safety foundation in california they might have something similar in washington try calling a motorcycle dealer in your area and asking its a good first start on how to ride a motorcycle correctly\n",
      "\n",
      "Rank 5: (Distance: 1.064742088317871)\n",
      "subject re first bike and wheelies organization st elizabeth hospital youngstown oh lines replyto john r daker nntppostinghost yfnysuedu in a previous article james leo belliveau says anyone i am a serious motorcycle enthusiast without a motorcycle and to put it bluntly it sucks i really would like some advice on what would be a good starter bike for me i do know one thing however i need to make my first bike a good one because buying a second any time soon is out of the question i am specifically interested in racing bikes cbr f gsxr i know that this may sound kind of crazy considering that ive never had a bike before but i am responsible a fast learner and in love please give me any advice that you think would help me in my search including places to look or even specific bikes that you want to sell me thanks the answer is obvious zx d dod darkman the significant problems we face cannot be solved at the same level of thinking we were at when we created them albert einstein the eternal champion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to perform a query using the Faiss index\n",
    "def search(query_text, k=5):\n",
    "    # Preprocess the query text\n",
    "    preprocessed_query = preprocess_text(query_text)\n",
    "    # Generate the query vector\n",
    "    query_vector = embed_text([preprocessed_query])\n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_vector.astype('float32'), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example Query\n",
    "query_text = \"motorcycle\"\n",
    "distances, indices = search(query_text)\n",
    "\n",
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Ensure that the displayed document is the preprocessed one\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{processed_documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dd94cba-715b-43fb-8de5-be14879e8653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: (Distance: 1.0367166996002197)\n",
      "From: James Leo Belliveau <jbc9+@andrew.cmu.edu>\n",
      "Subject: First Bike??\n",
      "Organization: Freshman, Mechanical Engineering, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 17\n",
      "NNTP-Posting-Host: andrew.cmu.edu\n",
      "\n",
      " Anyone, \n",
      "\n",
      "    I am a serious motorcycle enthusiast without a motorcycle, and to\n",
      "put it bluntly, it sucks.  I really would like some advice on what would\n",
      "be a good starter bike for me.  I do know one thing however, I need to\n",
      "make my first bike a good one, because buying a second any time soon is\n",
      "out of the question.  I am specifically interested in racing bikes, (CBR\n",
      "600 F2, GSX-R 750).  I know that this may sound kind of crazy\n",
      "considering that I've never had a bike before, but I am responsible, a\n",
      "fast learner, and in love.  Please give me any advice that you think\n",
      "would help me in my search, including places to look or even specific\n",
      "bikes that you want to sell me.\n",
      "\n",
      "    Thanks  :-)\n",
      "\n",
      "    Jamie Belliveau (jbc9@andrew.cmu.edu)  \n",
      "\n",
      "\n",
      "\n",
      "Rank 2: (Distance: 1.0392435789108276)\n",
      "From: James Leo Belliveau <jbc9+@andrew.cmu.edu>\n",
      "Subject: First Bike??\n",
      "Organization: Freshman, Mechanical Engineering, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 17\n",
      "NNTP-Posting-Host: po2.andrew.cmu.edu\n",
      "\n",
      " Anyone, \n",
      "\n",
      "    I am a serious motorcycle enthusiast without a motorcycle, and to\n",
      "put it bluntly, it sucks.  I really would like some advice on what would\n",
      "be a good starter bike for me.  I do know one thing however, I need to\n",
      "make my first bike a good one, because buying a second any time soon is\n",
      "out of the question.  I am specifically interested in racing bikes, (CBR\n",
      "600 F2, GSX-R 750).  I know that this may sound kind of crazy\n",
      "considering that I've never had a bike before, but I am responsible, a\n",
      "fast learner, and in love.  Please give me any advice that you think\n",
      "would help me in my search, including places to look or even specific\n",
      "bikes that you want to sell me.\n",
      "\n",
      "    Thanks  :-)\n",
      "\n",
      "    Jamie Belliveau (jbc9@andrew.cmu.edu)  \n",
      "\n",
      "\n",
      "\n",
      "Rank 3: (Distance: 1.0515626668930054)\n",
      "From: chrispi@microsoft.com (Chris Pirih)\n",
      "Subject: Re: First Bike??\n",
      "Organization: Microsoft Corporation\n",
      "Lines: 39\n",
      "\n",
      "In rec.motorcycles James Leo Belliveau <jbc9+@andrew.cmu.edu> writes:\n",
      ";    I am a serious motorcycle enthusiast without a motorcycle, and to\n",
      ";put it bluntly, it sucks.  I really would like some advice on what would\n",
      ";be a good starter bike for me.  I do know one thing however, I need to\n",
      ";make my first bike a good one, because buying a second any time soon is\n",
      ";out of the question.  I am specifically interested in racing bikes, (CBR\n",
      ";600 F2, GSX-R 750).  I know that this may sound kind of crazy\n",
      ";considering that I've never had a bike before, but I am responsible, a\n",
      ";fast learner, and in love.  \n",
      "\n",
      "Responsible and in love?  I believe that's a contradiction\n",
      "in terms.\n",
      "\n",
      "Unless you're really brave (read: \"reckless\") a 500cc sport\n",
      "bike will go way faster than you dare for at least your first\n",
      "year of riding.  Getting more than that really is overkill,\n",
      "as you'll never even want to use it.  The following bikes\n",
      "can be bought (and repaired!) cheaply, are easy for a novice\n",
      "to manage, and are plenty high performance:\n",
      "    Kawasaki EX-500\n",
      "    Honda VF-500 \"Interceptor\"\n",
      "    Suzuki GS-550E\n",
      "\n",
      "The 0-100mph time of the EX-500 at full throttle is \"way\n",
      "sooner than you're ready for it\".  :-)  With something\n",
      "as small as a 250, you'd probably be wishing for more\n",
      "power pretty quickly (unless it's a TZR or RGV :).\n",
      "\n",
      "Now, I'm not saying that you're 100% certain to kill\n",
      "yourself immediately with a 600f2 or a GSXR-750.  Plenty\n",
      "of people have started riding on those bikes and done\n",
      "just fine.  What I am saying is that it's a waste of\n",
      "money, and a waste of perfectly good plastic when you\n",
      "drop the thing learning how to balance while stopping.\n",
      "You'll never get the throttle more than half open\n",
      "anyway, so why spend the extra 2000 bucks?\n",
      "\n",
      "---\n",
      "chris\n",
      "\n",
      "\n",
      "Rank 4: (Distance: 1.0526401996612549)\n",
      "From: blaisec@sr.hp.com (Blaise Cirelli)\n",
      "Subject: Re: New to Motorcycles...\n",
      "Organization: HP Sonoma County (SRSD/MWTD/MID)\n",
      "X-Newsreader: TIN [version 1.1 PL8.8]\n",
      "Lines: 15\n",
      "\n",
      "Gregory Humphreys (gregh@niagara.dcrt.nih.gov) wrote:\n",
      "\n",
      "\n",
      "\n",
      "Greg,\n",
      "\n",
      "I'm very new to motorcycles. Haven't even bought one yet. I was in the same\n",
      "position about you. How do you learn if you've never ridden.\n",
      "\n",
      "I took a class put on by a group called the Motorcycle Safety Foundation\n",
      "in California. They might have something similar in Washington.\n",
      "\n",
      "Try calling a motorcycle dealer in your area and asking. It's a good first \n",
      "start on how to ride a motorcycle correctly.\n",
      "\n",
      "\n",
      "\n",
      "Rank 5: (Distance: 1.064742088317871)\n",
      "From: ak296@yfn.ysu.edu (John R. Daker)\n",
      "Subject: Re: First Bike?? and Wheelies\n",
      "Organization: St. Elizabeth Hospital, Youngstown, OH\n",
      "Lines: 24\n",
      "Reply-To: ak296@yfn.ysu.edu (John R. Daker)\n",
      "NNTP-Posting-Host: yfn.ysu.edu\n",
      "\n",
      "\n",
      "In a previous article, jbc9+@andrew.cmu.edu (James Leo Belliveau) says:\n",
      "\n",
      "> Anyone, \n",
      ">\n",
      ">    I am a serious motorcycle enthusiast without a motorcycle, and to\n",
      ">put it bluntly, it sucks.  I really would like some advice on what would\n",
      ">be a good starter bike for me.  I do know one thing however, I need to\n",
      ">make my first bike a good one, because buying a second any time soon is\n",
      ">out of the question.  I am specifically interested in racing bikes, (CBR\n",
      ">600 F2, GSX-R 750).  I know that this may sound kind of crazy\n",
      ">considering that I've never had a bike before, but I am responsible, a\n",
      ">fast learner, and in love.  Please give me any advice that you think\n",
      ">would help me in my search, including places to look or even specific\n",
      ">bikes that you want to sell me.\n",
      ">\n",
      ">    Thanks  :-)\n",
      "\n",
      "The answer is obvious: ZX-11 D.\n",
      "-- \n",
      "DoD #650<----------------------------------------------------------->DarkMan\n",
      "   The significant problems we face cannot be solved at the same level of\n",
      "      thinking we were at when we created them.   - Albert Einstein\n",
      "         ___________________The Eternal Champion_________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Displaying the original (unprocessed) document corresponding to the search result\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a617fc-22a8-4ae8-b8d3-1697f7b3035e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
